{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cee15ac-ddf4-4da4-859e-3db5ea5cfddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import pandas as pd\n",
    "import sys\n",
    "from azure.eventhub import EventHubConsumerClient\n",
    "from pyspark.sql import SparkSession # Initializing SparkSession\n",
    "\n",
    "# Define Event Hub Connection Parameters\n",
    "eventhub_namespace = \"factored-datathon\"\n",
    "eventhub_name = \"factored_datathon_amazon_reviews_1\"\n",
    "listen_policy_key = \"2GETvVt0FxyM0bo0Qau4inlmC/w3t4Uut+AEhAnAEgk=\"\n",
    "listen_policy_connection_string = \"Endpoint=sb://factored-datathon.servicebus.windows.net/;SharedAccessKeyName=datathon_group_1;SharedAccessKey=2GETvVt0FxyM0bo0Qau4inlmC/w3t4Uut+AEhAnAEgk=;\"\n",
    "i=0\n",
    "received=[]\n",
    "df = {}\n",
    "# Define the Event Hub Consumer Function and processing logic for each incoming event\n",
    "def on_event(partition_context, event):\n",
    "    global i \n",
    "    global received\n",
    "    global df\n",
    "    dato=json.loads(event.body_as_str(encoding='UTF-8'))\n",
    "    \n",
    "    df[i] = dato\n",
    "\n",
    "    if dato['partition_number'] in received:\n",
    "        sys.exit()\n",
    "    received.append(dato['partition_number'])\n",
    "    i += 1\n",
    "\n",
    "# Set up the Event Hub Consumer Client\n",
    "connection_str = listen_policy_connection_string\n",
    "consumer_client = EventHubConsumerClient.from_connection_string(connection_str, consumer_group=\"data_hackers\", eventhub_name=eventhub_name,idle_timeout=120)\n",
    "\n",
    "# Start Receiving Events\n",
    "try:\n",
    "    with consumer_client:\n",
    "        consumer_client.receive(on_event=on_event, starting_position=\"-1\")\n",
    "except SystemExit:\n",
    "    spark.createDataFrame(pd.DataFrame.from_dict(df, orient='index')).write.saveAsTable(\"batch_stream\") \n",
    "    print(\"Receiving has stopped.\",i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84c8a42-06b7-4bb4-807b-85ac4e4395cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "\n",
    "def authenticate_implicit_with_adc(project_id=\"proyectozoomcamp\"):\n",
    "    \"\"\"\n",
    "    When interacting with Google Cloud Client libraries, the library can auto-detect the\n",
    "    credentials to use.\n",
    "\n",
    "    // TODO(Developer):\n",
    "    //  1. Before running this sample,\n",
    "    //  set up ADC as described in https://cloud.google.com/docs/authentication/external/set-up-adc\n",
    "    //  2. Replace the project variable.\n",
    "    //  3. Make sure that the user account or service account that you are using\n",
    "    //  has the required permissions. For this sample, you must have \"storage.buckets.list\".\n",
    "    Args:\n",
    "        project_id: The project id of your Google Cloud project.\n",
    "    \"\"\"\n",
    "\n",
    "    # This snippet demonstrates how to list buckets.\n",
    "    # *NOTE*: Replace the client created below with the client required for your application.\n",
    "    # Note that the credentials are not specified when constructing the client.\n",
    "    # Hence, the client library will look for credentials using ADC.\n",
    "    storage_client = storage.Client(project=project_id)\n",
    "    buckets = storage_client.list_buckets()\n",
    "    print(\"Buckets:\")\n",
    "    for bucket in buckets:\n",
    "        print(bucket.name)\n",
    "    print(\"Listed all storage buckets.\")\n",
    "authenticate_implicit_with_adc(project_id=\"proyectozoomcamp\")\n",
    "storage_client = storage.Client(project=\"proyectozoomcamp\")\n",
    "lista_archivos=[]\n",
    "# Create a bucket object for our bucket\n",
    "bucket = storage_client.get_bucket('datathon-23')\n",
    "for blob in storage_client.list_blobs('datathon-23', prefix='amazon_reviews'):\n",
    "  lista_archivos.append(blob.name)\n",
    "indices = [i for i, x in enumerate(lista_archivos) if \".json.gz\" in x]\n",
    "lista_archivos=[lista_archivos[i] for i in indices]\n",
    "for i, x in enumerate(lista_archivos):\n",
    "    if i<475:\n",
    "        pass\n",
    "    else:\n",
    "        blob = bucket.blob(x)\n",
    "        if x[7]==\"m\":\n",
    "            nombre=\"/home/r540/Datathon/data/metadata/{}.json.gz\".format(i)\n",
    "        else:\n",
    "            nombre=\"/home/r540/Datathon/data/reviews/{}.json.gz\".format(i)\n",
    "        # Download the file to a destination\n",
    "    \n",
    "        blob.download_to_filename(nombre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c07ad74-912b-483e-9742-36cb70453d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### to connect to streaming data\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "from confluent_kafka import Consumer\n",
    "from ast import literal_eval\n",
    "consumer = Consumer(\n",
    "         {'bootstrap.servers': 'pkc-41973.westus2.azure.confluent.cloud:9092',\n",
    "        \"group.id\": \"group_data_h\",\n",
    "        'security.protocol':'SASL_SSL',\n",
    "        'sasl.mechanisms':'PLAIN',\n",
    "        'sasl.username':'xxx',\n",
    "        'sasl.password':'xxx',\n",
    "        'auto.offset.reset': 'earliest',\n",
    "        'enable.auto.commit': True })\n",
    "\n",
    "consumer.subscribe(['factored_datathon_amazon_review_1'])\n",
    "i=0\n",
    "received=[]\n",
    "df = {}\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        msg = consumer.poll(1.0)\n",
    "        print(msg)\n",
    "        if msg is None:\n",
    "            continue\n",
    "        user = msg.value()\n",
    "        if user is not None:\n",
    "            nus=literal_eval(user.decode('utf8'))\n",
    "            print(nus)\n",
    "            dato=json.loads(json.dumps(nus, indent=4))\n",
    "            df[i] = dato\n",
    "            output_path=\"/home/r540/Datathon/data/stream/stream.csv\"\n",
    "            pd.DataFrame.from_dict(df, orient='index').to_csv(output_path, mode='a', header=not os.path.exists(output_path),index=False)\n",
    "            if dato['partition_number'] in received:\n",
    "                sys.exit()\n",
    "            received.append(dato['partition_number'])\n",
    "            i += 1\n",
    "            \n",
    "except SystemExit:\n",
    "  print('closing the consumer')\n",
    "  consumer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
